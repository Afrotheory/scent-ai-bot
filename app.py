import streamlit as st
import google.generativeai as genai

# 1. é¡µé¢é…ç½®
st.set_page_config(page_title="Scent Curator Assistant", layout="centered")
st.title("ğŸ¯ ä¸œæ–¹é¦™ç¤¼è·¨å¢ƒè¥é”€åŠ©æ‰‹ (ä¸“ä¸šç‰ˆ)")

# 2. åˆå§‹åŒ–å¯¹è¯è®°å¿† (Multi-turn Chat)
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. è¯»å–çŸ¥è¯†åº“ (å¢åŠ ç¼“å­˜æé«˜æ€§èƒ½)
@st.cache_data
def load_docs():
    try:
        with open("docs/SOP_Flow.md", "r", encoding="utf-8") as f:
            sop = f.read()
        with open("docs/Product_Info.md", "r", encoding="utf-8") as f:
            product = f.read()
        return sop, product
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return "", ""

sop_content, product_content = load_docs()

# 4. é…ç½® Gemini API
gemini_key = st.secrets.get("GEMINI_API_KEY")

if not gemini_key:
    st.error("è¯·åœ¨ Streamlit Secrets ä¸­é…ç½® GEMINI_API_KEY")
else:
    genai.configure(api_key=gemini_key)

    # è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ Gemini æ¨¡å‹åç§° (ä¿®å¤404é—®é¢˜)
    try:
        available_models = [m.name for m in genai.list_models() if "generateContent" in m.supported_generation_methods]
        target_model = next((m for m in available_models if "gemini-1.5-flash" in m), available_models[0])
        model = genai.GenerativeModel(model_name=target_model)
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        model = None

    # 5. ä¾§è¾¹æ æ§åˆ¶
    with st.sidebar:
        st.header("æ§åˆ¶é¢æ¿")
        if st.button("æ¸…é™¤å¯¹è¯è®°å½•"):
            st.session_state.messages = []
            st.rerun()
        st.markdown("---")
        st.info("ğŸ’¡ ç³»ç»Ÿå·²åŠ è½½ SOP ä¸äº§å“æ‰‹å†Œï¼Œç°åœ¨æ‹¥æœ‰ä¸Šä¸‹æ–‡è®°å¿†ã€‚")

    # 6. æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 7. ç”¨æˆ·è¾“å…¥é€»è¾‘
    if prompt_input := st.chat_input("ç²˜è´´å®¢æˆ·çš„è¯..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt_input)
        st.session_state.messages.append({"role": "user", "content": prompt_input})

        # 8. æ ¸å¿ƒ Prompt æ•´åˆ
        # ç™‚ç™’å¸«è§’è‰²ã€æœ›èå•åˆ‡ã€ç ´å†°è¿½å•ã€è½‰åŒ–æ„åœ–
      system_instruction = f"""
        You are an "Eastern Scent Therapist." Your goal: Build trust via concise, professional diagnosis. 

        ã€Core Knowledgeã€‘
        SOP: {sop_content}
        Products: {product_content}

        ã€Communication Rules - MANDATORYã€‘
        1. STRIKE THE CHAT-KILLER: No long paragraphs. English replies MUST be 1-3 short, natural sentences.
        2. DIAGNOSIS (æœ›èå•åˆ‡): If a symptom is mentioned, ask ONLY ONE specific follow-up question (e.g., "Is the pain worse in the morning?" or "Is it hard to fall asleep or stay asleep?").
        3. CHASE-UP STRATEGY: Provide a ultra-short (max 2 sentences) follow-up text with its Chinese translation.
        4. TRANSLATION: Every English text provided must have a corresponding Chinese translation.

        ã€Output Structure - Follow Strictlyã€‘
        ### 1. ç™‚ç™’å¸«å…§éƒ¨ç­–ç•¥ (Internal Analysis)
        - **SOP éšæ®µ**: [ç›®å‰éšæ®µ]
        - **æœ›èå•åˆ‡ (Diagnosis)**: [åˆ†æç—‡ç‹€ï¼Œä¸¦çµ¦å‡ºä¸€å€‹ç²¾æº–çš„ã€Œå°ˆæ¥­è©¢å•é»ã€]
        - **è¿½å•ç­–ç•¥ (Chase-up)**: [è‹¥å®¢æˆ¶æ²’å›ï¼Œéš”å¤©å¯ç”¨çš„ã€Œ1-2å¥ã€çŸ­å¥]
        - **ç­–ç•¥ä¸­æ–‡æ„åœ–**: [ä¸­æ–‡ç¿»è­¯åŠç‚ºä»€éº¼é€™æ¨£èƒ½è§¸é”å®¢æˆ¶]

        ### 2. å»ºè­°è‹±æ–‡å›è¦† (The Mentor's Reply)
        [1-3 sentences of elegant English. Must end with a gentle question.]

        ### 3. ä¸­æ–‡åƒè€ƒ (Translation)
        [ä¸Šè¿°è‹±æ–‡å›è¦†çš„å°æ‡‰ä¸­æ–‡]
        """

        # è·å–æœ€è¿‘å‡ è½®å¯¹è¯ä¸Šä¸‹æ–‡
        context_history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:]])
        final_prompt = f"{system_instruction}\n\nRecent History:\n{context_history}\n\nLatest Query: {prompt_input}"

        # 9. è°ƒç”¨ AI ç”Ÿæˆå›å¤
        if model:
            with st.chat_message("assistant"):
                with st.spinner("æ­£åœ¨æ€è€ƒæœ€åœ°é“çš„è¡¨è¾¾..."):
                    try:
                        response = model.generate_content(final_prompt)
                        full_response = response.text
                        st.markdown(full_response)
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
