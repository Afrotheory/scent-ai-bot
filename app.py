import streamlit as st
import google.generativeai as genai

# 1. é¡µé¢é…ç½®
st.set_page_config(page_title="Scent Curator Assistant", layout="centered")
st.title("ğŸ¯ ä¸œæ–¹é¦™ç¤¼è·¨å¢ƒè¥é”€åŠ©æ‰‹ (ä¸“ä¸šç‰ˆ)")

# 2. åˆå§‹åŒ–å¯¹è¯è®°å¿† (Multi-turn Chat)
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. è¯»å–çŸ¥è¯†åº“ (å¢åŠ ç¼“å­˜æé«˜æ€§èƒ½)
@st.cache_data
def load_docs():
    try:
        with open("docs/SOP_Flow.md", "r", encoding="utf-8") as f:
            sop = f.read()
        with open("docs/Product_Info.md", "r", encoding="utf-8") as f:
            product = f.read()
        return sop, product
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return "", ""

sop_content, product_content = load_docs()

# 4. é…ç½® Gemini API
gemini_key = st.secrets.get("GEMINI_API_KEY")

if not gemini_key:
    st.error("è¯·åœ¨ Streamlit Secrets ä¸­é…ç½® GEMINI_API_KEY")
else:
    genai.configure(api_key=gemini_key)

    # è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ Gemini æ¨¡å‹åç§° (ä¿®å¤404é—®é¢˜)
    try:
        available_models = [m.name for m in genai.list_models() if "generateContent" in m.supported_generation_methods]
        target_model = next((m for m in available_models if "gemini-1.5-flash" in m), available_models[0])
        model = genai.GenerativeModel(model_name=target_model)
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        model = None

    # 5. ä¾§è¾¹æ æ§åˆ¶
    with st.sidebar:
        st.header("æ§åˆ¶é¢æ¿")
        if st.button("æ¸…é™¤å¯¹è¯è®°å½•"):
            st.session_state.messages = []
            st.rerun()
        st.markdown("---")
        st.info("ğŸ’¡ ç³»ç»Ÿå·²åŠ è½½ SOP ä¸äº§å“æ‰‹å†Œï¼Œç°åœ¨æ‹¥æœ‰ä¸Šä¸‹æ–‡è®°å¿†ã€‚")

    # 6. æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 7. ç”¨æˆ·è¾“å…¥é€»è¾‘
    if prompt_input := st.chat_input("ç²˜è´´å®¢æˆ·çš„è¯..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt_input)
        st.session_state.messages.append({"role": "user", "content": prompt_input})

        # 8. æ ¸å¿ƒ Prompt æ•´åˆ
        # ç™‚ç™’å¸«è§’è‰²ã€æœ›èå•åˆ‡ã€ç ´å†°è¿½å•ã€è½‰åŒ–æ„åœ–
        system_instruction = f"""
        You are a "Scent Healing Mentor" (Eastern Scent Therapist). Your goal is not to sell, but to "diagnose and cure" through scents, building a deep bond with the customer.

        ã€Knowledge Baseã€‘
        SOP: {sop_content}
        Products: {product_content}

        ã€Instructions for Professionalism & Conversionã€‘
        1. THE SKEPTIC'S GUIDE: If a customer is skeptical, explain the "Transdermal Absorption" and "Olfactory Neural Response" in a sophisticated way (Botanical energy interacting with body heat).
        2. VASCULAR & NERVOUS FOCUS (æœ›èå•åˆ‡): When symptoms are mentioned, DO NOT jump to products. First, generate 1-2 caring follow-up questions to understand their lifestyle (e.g., stress levels, sleep patterns, or pain triggers).
        3. ANTI-CHAT-END: Every reply must end with a gentle question or an emotional hook to keep the conversation alive.
        4. ROLE: You are an expert friend. Use "I've seen similar cases...", "In our tradition, we believe...", "Actually, your body is telling you...".

        ã€Output Structureã€‘
        ### 1. ç™‚ç™’å¸«å…§éƒ¨è¨ºæ–· (Internal Strategy)
        - **SOP éšæ®µ**: [ç›®å‰è™•æ–¼SOPå“ªä¸€ç’°]
        - **æœ›èå•åˆ‡ (The Diagnosis)**: [åˆ†æç—‡ç‹€èƒŒå¾Œçš„æˆå› ï¼Œä¸¦çµ¦å‡º1-2å€‹é€²ä¸€æ­¥è©¢å•å®¢æˆ¶çš„é»]
        - **ç ´å†°èˆ‡è¿½å•ç­–ç•¥**: [å¦‚æœå®¢æˆ¶é€™æ¢æ²’å›ï¼Œä½ æ˜å¤©è©²å¦‚ä½•ç”¨ä»€éº¼è©±é¡Œé‡æ–°æ“Šä¸­ä»–çš„ç—›é»ï¼Ÿ]

        ### 2. å»ºè­°è‹±æ–‡å›è¦† (The Scent Mentor's Reply)
        [Start with empathy -> Brief professional insight (Cold-infusion logic) -> Soft product hint -> A gentle follow-up question]

        ### 3. ä¸­æ–‡å°ç…§èˆ‡æ„åœ– (Translation & Intent)
        [ä¸­æ–‡ç¿»è­¯ï¼Œä¸¦èªªæ˜ç‚ºä»€éº¼é€™æ¨£å›è¦†èƒ½å¼•å°è½‰åŒ–]
        """

        # è·å–æœ€è¿‘å‡ è½®å¯¹è¯ä¸Šä¸‹æ–‡
        context_history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:]])
        final_prompt = f"{system_instruction}\n\nRecent History:\n{context_history}\n\nLatest Query: {prompt_input}"

        # 9. è°ƒç”¨ AI ç”Ÿæˆå›å¤
        if model:
            with st.chat_message("assistant"):
                with st.spinner("æ­£åœ¨æ€è€ƒæœ€åœ°é“çš„è¡¨è¾¾..."):
                    try:
                        response = model.generate_content(final_prompt)
                        full_response = response.text
                        st.markdown(full_response)
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
