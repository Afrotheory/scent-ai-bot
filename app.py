import streamlit as st
import google.generativeai as genai

# 1. é é¢é…ç½®
st.set_page_config(page_title="Scent Curator Assistant", layout="centered")
st.title("ğŸ¯ æ±æ–¹é¦™ç¦®è·¨å¢ƒç‡ŸéŠ·åŠ©æ‰‹ (å°ˆæ¥­ç‰ˆ)")

# 2. åˆå§‹åŒ–å°è©±è¨˜æ†¶
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. è®€å–çŸ¥è­˜åº«
@st.cache_data
def load_docs():
    try:
        with open("docs/SOP_Flow.md", "r", encoding="utf-8") as f:
            sop = f.read()
        with open("docs/Product_Info.md", "r", encoding="utf-8") as f:
            product = f.read()
        return sop, product
    except Exception as e:
        st.error(f"è®€å–æ–‡ä»¶å¤±æ•—: {e}")
        return "", ""

sop_content, product_content = load_docs()

# 4. é…ç½® Gemini
gemini_key = st.secrets.get("GEMINI_API_KEY")
if not gemini_key:
    st.error("è«‹åœ¨ Secrets ä¸­é…ç½® GEMINI_API_KEY")
else:
    genai.configure(api_key=gemini_key)
    model = genai.GenerativeModel('gemini-1.5-flash')

    with st.sidebar:
        if st.button("æ¸…é™¤å°è©±è¨˜éŒ„ (Clear Chat)"):
            st.session_state.messages = []
            st.rerun()

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("åœ¨æ­¤ç²˜è²¼å®¢æˆ¶çš„è©±..."):
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # --- æ ¸å¿ƒæŒ‡ä»¤é‡æ§‹ (æ³¨æ„é€™è£¡çš„ç¸®é€²) ---
        system_instruction = f"""
        You are an "Eastern Scent Therapist." Your goal: Build trust via concise, professional diagnosis. 

        ã€Core Knowledgeã€‘
        SOP: {sop_content}
        Products: {product_content}

        ã€Communication Rules - MANDATORYã€‘
        1. STRIKE THE CHAT-KILLER: No long paragraphs. English replies MUST be 1-3 short, natural sentences.
        2. DIAGNOSIS (æœ›èå•åˆ‡): If a symptom is mentioned, ask ONLY ONE specific follow-up question.
        3. CHASE-UP STRATEGY: Provide a ultra-short (max 2 sentences) follow-up text with its Chinese translation.
        4. TRANSLATION: Every English text provided must have a corresponding Chinese translation.

        ã€Output Structure - Follow Strictlyã€‘
        ### 1. ç™‚ç™’å¸«å…§éƒ¨ç­–ç•¥ (Internal Analysis)
        - **SOP éšæ®µ**: [ç›®å‰éšæ®µ]
        - **æœ›èå•åˆ‡ (Diagnosis)**: [åˆ†æç—‡ç‹€ï¼Œä¸¦çµ¦å‡ºä¸€å€‹ç²¾æº–çš„ã€Œå°ˆæ¥­è©¢å•é»ã€]
        - **è¿½å•ç­–ç•¥ (Chase-up)**: [è‹¥å®¢æˆ¶æ²’å›ï¼Œéš”å¤©å¯ç”¨çš„ã€Œ1-2å¥ã€çŸ­å¥]
        - **ç­–ç•¥ä¸­æ–‡æ„åœ–**: [ä¸­æ–‡ç¿»è­¯åŠç‚ºä»€éº¼é€™æ¨£èƒ½è§¸é”å®¢æˆ¶]

        ### 2. å»ºè­°è‹±æ–‡å›è¦† (The Mentor's Reply)
        [1-3 sentences of elegant English. Must end with a gentle question.]

        ### 3. ä¸­æ–‡åƒè€ƒ (Translation)
        [ä¸Šè¿°è‹±æ–‡å›è¦†çš„å°æ‡‰ä¸­æ–‡]
        """

        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                try:
                    history_context = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:]])
                    full_prompt = f"{system_instruction}\n\nHistory:\n{history_context}\n\nLatest Query: {prompt}"
                    
                    response = model.generate_content(full_prompt)
                    answer = response.text
                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    st.error(f"ç”Ÿæˆå¤±æ•—: {e}")
