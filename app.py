import streamlit as st
import google.generativeai as genai

# 1. é¡µé¢é…ç½®
st.set_page_config(page_title="Scent Curator Assistant", layout="centered")
st.title("ğŸ¯ ä¸œæ–¹é¦™ç¤¼è·¨å¢ƒè¥é”€åŠ©æ‰‹ (ä¸“ä¸šç‰ˆ)")

# 2. åˆå§‹åŒ–å¯¹è¯è®°å¿† (Multi-turn Chat)
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. è¯»å–çŸ¥è¯†åº“ (å¢åŠ ç¼“å­˜æé«˜æ€§èƒ½)
@st.cache_data
def load_docs():
    try:
        with open("docs/SOP_Flow.md", "r", encoding="utf-8") as f:
            sop = f.read()
        with open("docs/Product_Info.md", "r", encoding="utf-8") as f:
            product = f.read()
        return sop, product
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return "", ""

sop_content, product_content = load_docs()

# 4. é…ç½® Gemini API
gemini_key = st.secrets.get("GEMINI_API_KEY")

if not gemini_key:
    st.error("è¯·åœ¨ Streamlit Secrets ä¸­é…ç½® GEMINI_API_KEY")
else:
    genai.configure(api_key=gemini_key)

    # è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ Gemini æ¨¡å‹åç§° (ä¿®å¤404é—®é¢˜)
    try:
        available_models = [m.name for m in genai.list_models() if "generateContent" in m.supported_generation_methods]
        target_model = next((m for m in available_models if "gemini-1.5-flash" in m), available_models[0])
        model = genai.GenerativeModel(model_name=target_model)
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        model = None

    # 5. ä¾§è¾¹æ æ§åˆ¶
    with st.sidebar:
        st.header("æ§åˆ¶é¢æ¿")
        if st.button("æ¸…é™¤å¯¹è¯è®°å½•"):
            st.session_state.messages = []
            st.rerun()
        st.markdown("---")
        st.info("ğŸ’¡ ç³»ç»Ÿå·²åŠ è½½ SOP ä¸äº§å“æ‰‹å†Œï¼Œç°åœ¨æ‹¥æœ‰ä¸Šä¸‹æ–‡è®°å¿†ã€‚")

    # 6. æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 7. ç”¨æˆ·è¾“å…¥é€»è¾‘
    if prompt_input := st.chat_input("ç²˜è´´å®¢æˆ·çš„è¯..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt_input)
        st.session_state.messages.append({"role": "user", "content": prompt_input})

        # 8. æ ¸å¿ƒ Prompt æ•´åˆ
        # åŒ…å«äº†å“ç‰Œè¯­è°ƒã€SOPé€»è¾‘ã€æ„Ÿå®˜å™äº‹è¦æ±‚
        system_instruction = f"""
        You are a sophisticated Scent Curator and Wellness Consultant for "Cold-Infused Incense".
        Your tone is Elegant, Professional, Empathetic, and Zen-like.

        ã€Knowledge Baseã€‘
        SOP Guidance: {sop_content}
        Product Catalog & Stories: {product_content}

        ã€Instruction Rulesã€‘
        1. CONCISE: English replies must be warm and human-like, within 1-3 sentences.
        2. NO JARGON: Use "Ancient Artisan Craft", "Living Scent", "Bespoke Consultation".
        3. NO HARD SELL: Be a helpful, knowledgeable friend first.
        4. STRUCTURE: You MUST respond in this format:
           ### 1. å†…éƒ¨é€»è¾‘åˆ†æ (Internal Analysis)
           - SOPé˜¶æ®µ: [åˆ¤æ–­é˜¶æ®µ]
           - æ„å›¾åˆ†æ: [åˆ†æå®¢æˆ·ç—›ç‚¹ä¸å¿ƒç†]

           ### 2. å»ºè®®å›å¤ (English Reply)
           [Elegant & Natural English Reply]

           ### 3. ä¸­æ–‡å¯¹åº”å‚è€ƒ (Translation)
           [ä¸­æ–‡ç¿»è¯‘]
        """

        # è·å–æœ€è¿‘å‡ è½®å¯¹è¯ä¸Šä¸‹æ–‡
        context_history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:]])
        final_prompt = f"{system_instruction}\n\nRecent History:\n{context_history}\n\nLatest Query: {prompt_input}"

        # 9. è°ƒç”¨ AI ç”Ÿæˆå›å¤
        if model:
            with st.chat_message("assistant"):
                with st.spinner("æ­£åœ¨æ€è€ƒæœ€åœ°é“çš„è¡¨è¾¾..."):
                    try:
                        response = model.generate_content(final_prompt)
                        full_response = response.text
                        st.markdown(full_response)
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
